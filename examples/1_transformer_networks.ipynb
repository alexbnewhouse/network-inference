{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925da2bb",
   "metadata": {},
   "source": [
    "# Transformer-Based Semantic Networks\n",
    "\n",
    "This notebook demonstrates how to build semantic networks using transformer models (sentence embeddings) instead of traditional co-occurrence methods.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Setup** - Install dependencies and import libraries\n",
    "2. **Basic Concepts** - Understanding sentence embeddings and similarity\n",
    "3. **Core Implementation** - Building document and term networks\n",
    "4. **Practical Examples** - Real-world use cases with sample data\n",
    "5. **Visualization & Analysis** - Comparing models and analyzing results\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8+\n",
    "- `sentence-transformers` package\n",
    "- `scikit-learn` for cosine similarity\n",
    "- `networkx` for network analysis\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda60f2b",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and verify our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import our transformer modules\n",
    "from src.semantic.transformers_enhanced import (\n",
    "    TransformerEmbeddings,\n",
    "    TransformerSemanticNetwork\n",
    ")\n",
    "\n",
    "# NetworkX for graph analysis\n",
    "import networkx as nx\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(f\"‚úì Python version: {sys.version}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")\n",
    "print(f\"‚úì Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb7152",
   "metadata": {},
   "source": [
    "## Section 2: Basic Concepts - Understanding Sentence Embeddings\n",
    "\n",
    "### What are Sentence Embeddings?\n",
    "\n",
    "Sentence embeddings are dense vector representations of text that capture semantic meaning. Unlike traditional co-occurrence methods (which count word frequencies), embeddings represent text in a continuous vector space where similar meanings are close together.\n",
    "\n",
    "**Key Advantages:**\n",
    "- üéØ Capture semantic similarity (e.g., \"car\" and \"automobile\" are close)\n",
    "- üåç Work across different phrasings and synonyms\n",
    "- üìä Produce dense vectors (384-768 dimensions) vs. sparse co-occurrence matrices\n",
    "- üöÄ Pre-trained on massive corpora\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db592873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer embeddings model\n",
    "# Using MiniLM (small, fast, 384 dimensions)\n",
    "embedder = TransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on the rug.\",  # Semantically similar to #1\n",
    "    \"Python is a programming language.\",\n",
    "    \"Machine learning uses neural networks.\",\n",
    "    \"The dog played in the park.\"\n",
    "]\n",
    "\n",
    "# Encode sentences to embeddings\n",
    "embeddings = embedder.encode(sentences)\n",
    "\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "print(f\"  ‚Üí {len(sentences)} sentences\")\n",
    "print(f\"  ‚Üí {embeddings.shape[1]} dimensions per sentence\")\n",
    "print(f\"\\nFirst embedding (first 10 dimensions):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5102dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix (cosine similarity)\n",
    "similarity_matrix = embedder.compute_similarity_matrix(sentences)\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarity_matrix, \n",
    "    annot=True, \n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    xticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "    yticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "    vmin=0, \n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Cosine Similarity'}\n",
    ")\n",
    "plt.title('Sentence Similarity Matrix\\n(Notice how S1 and S2 are highly similar!)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the actual sentences for reference\n",
    "print(\"\\nSentence Reference:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"S{i+1}: {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea8d7d",
   "metadata": {},
   "source": [
    "## Section 3: Core Implementation - Building Networks\n",
    "\n",
    "Now let's build actual semantic networks! We'll create two types:\n",
    "1. **Document Network**: Connect similar documents\n",
    "2. **Term Network**: Connect similar terms/phrases\n",
    "\n",
    "### 3.1 Document Network\n",
    "\n",
    "This creates a network where each node is a document, and edges connect similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5596fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for a larger dataset\n",
    "documents = [\n",
    "    \"Climate change is affecting global temperatures and weather patterns.\",\n",
    "    \"Global warming leads to rising sea levels and extreme weather events.\",\n",
    "    \"Machine learning algorithms can predict patterns in large datasets.\",\n",
    "    \"Neural networks are the foundation of modern AI systems.\",\n",
    "    \"Deep learning models require significant computational resources.\",\n",
    "    \"Electric vehicles are becoming more popular as battery technology improves.\",\n",
    "    \"Tesla and other companies are investing heavily in EV infrastructure.\",\n",
    "    \"Renewable energy sources like solar and wind power are growing rapidly.\",\n",
    "    \"Solar panels convert sunlight directly into electricity.\",\n",
    "    \"The stock market experienced significant volatility last quarter.\",\n",
    "    \"Cryptocurrency prices fluctuate based on market sentiment and adoption.\",\n",
    "    \"Bitcoin and Ethereum are the most well-known cryptocurrencies.\",\n",
    "]\n",
    "\n",
    "# Initialize the network builder\n",
    "network_builder = TransformerSemanticNetwork(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Build document network\n",
    "doc_edges = network_builder.build_document_network(\n",
    "    documents=documents,\n",
    "    similarity_threshold=0.3,  # Connect docs with >30% similarity\n",
    "    top_k=5  # Keep top 5 connections per document\n",
    ")\n",
    "\n",
    "print(f\"Document Network Created!\")\n",
    "print(f\"  Documents: {len(documents)}\")\n",
    "print(f\"  Edges: {len(doc_edges)}\")\n",
    "print(f\"\\nFirst few edges:\")\n",
    "print(doc_edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b80025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graph for visualization\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges from our document network\n",
    "for _, row in doc_edges.iterrows():\n",
    "    G.add_edge(\n",
    "        row['source'], \n",
    "        row['target'], \n",
    "        weight=row['similarity']\n",
    "    )\n",
    "\n",
    "# Visualize the network\n",
    "plt.figure(figsize=(14, 10))\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Draw edges with varying thickness based on similarity\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos, \n",
    "    width=[w*3 for w in weights],\n",
    "    alpha=0.3,\n",
    "    edge_color='gray'\n",
    ")\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(\n",
    "    G, pos,\n",
    "    node_color='lightblue',\n",
    "    node_size=800,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(\n",
    "    G, pos,\n",
    "    font_size=10,\n",
    "    font_weight='bold'\n",
    ")\n",
    "\n",
    "plt.title('Document Similarity Network\\n(Thickness = Similarity Strength)', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNetwork Statistics:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Avg Degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9acaf",
   "metadata": {},
   "source": [
    "### 3.2 Term Network\n",
    "\n",
    "Now let's build a network of terms/phrases instead of documents. This is useful for understanding which concepts are semantically related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key terms/concepts to analyze\n",
    "terms = [\n",
    "    \"artificial intelligence\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"neural networks\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision\",\n",
    "    \"climate change\",\n",
    "    \"global warming\",\n",
    "    \"renewable energy\",\n",
    "    \"solar power\",\n",
    "    \"electric vehicles\",\n",
    "    \"cryptocurrency\",\n",
    "    \"blockchain\",\n",
    "    \"bitcoin\",\n",
    "    \"stock market\",\n",
    "    \"financial markets\",\n",
    "]\n",
    "\n",
    "# Build term network\n",
    "term_edges = network_builder.build_term_network(\n",
    "    terms=terms,\n",
    "    similarity_threshold=0.4,  # Higher threshold for terms\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Term Network Created!\")\n",
    "print(f\"  Terms: {len(terms)}\")\n",
    "print(f\"  Edges: {len(term_edges)}\")\n",
    "print(f\"\\nStrongest connections:\")\n",
    "print(term_edges.nlargest(10, 'similarity')[['source', 'target', 'similarity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb3850",
   "metadata": {},
   "source": [
    "## Section 4: Practical Examples - Real-World Use Cases\n",
    "\n",
    "Let's demonstrate practical applications with more realistic data scenarios.\n",
    "\n",
    "### Example 1: Topic Clustering from News Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate news headlines dataset\n",
    "news_headlines = [\n",
    "    \"Tech Giants Invest Billions in AI Research and Development\",\n",
    "    \"New AI Model Breaks Records in Language Understanding Tasks\",\n",
    "    \"Stock Market Hits All-Time High Amid Economic Recovery\",\n",
    "    \"Cryptocurrency Regulation Debate Intensifies in Congress\",\n",
    "    \"Bitcoin Price Surges Following Institutional Adoption\",\n",
    "    \"Climate Summit Reaches Historic Agreement on Emissions\",\n",
    "    \"Scientists Warn of Accelerating Global Temperature Rise\",\n",
    "    \"Renewable Energy Capacity Doubles in Past Five Years\",\n",
    "    \"Solar Panel Efficiency Reaches New Milestone in Labs\",\n",
    "    \"Electric Vehicle Sales Outpace Traditional Cars in Europe\",\n",
    "    \"Tesla Opens New Gigafactory to Meet Growing Demand\",\n",
    "    \"Medical AI Diagnoses Diseases with 95% Accuracy\",\n",
    "    \"Quantum Computing Breakthrough Announced by Researchers\",\n",
    "    \"Space Agency Plans Mission to Mars by 2030\",\n",
    "    \"Ocean Plastic Cleanup Project Exceeds Expectations\",\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': range(len(news_headlines)),\n",
    "    'text': news_headlines\n",
    "})\n",
    "\n",
    "print(\"Sample Headlines:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73458388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network from news headlines\n",
    "news_edges = network_builder.build_document_network(\n",
    "    documents=news_headlines,\n",
    "    similarity_threshold=0.25,\n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "# Create graph\n",
    "G_news = nx.Graph()\n",
    "for _, row in news_edges.iterrows():\n",
    "    G_news.add_edge(row['source'], row['target'], weight=row['similarity'])\n",
    "\n",
    "# Detect communities using Louvain algorithm\n",
    "from networkx.algorithms import community\n",
    "\n",
    "communities = community.greedy_modularity_communities(G_news)\n",
    "\n",
    "print(f\"\\nDetected {len(communities)} topic clusters:\")\n",
    "for i, comm in enumerate(communities):\n",
    "    print(f\"\\nüì∞ Cluster {i+1}:\")\n",
    "    for doc_id in sorted(comm):\n",
    "        print(f\"   [{doc_id}] {news_headlines[doc_id][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6a689",
   "metadata": {},
   "source": [
    "### Example 2: Finding Similar Documents\n",
    "\n",
    "Let's say you have a query document and want to find the most similar documents in your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b493a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query document\n",
    "query = \"Artificial intelligence is transforming healthcare and medical diagnostics\"\n",
    "\n",
    "# Encode query and all documents\n",
    "query_embedding = embedder.encode([query])\n",
    "doc_embeddings = embedder.encode(news_headlines)\n",
    "\n",
    "# Compute similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Get top 5 most similar\n",
    "top_indices = similarities.argsort()[-5:][::-1]\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nüîç Top 5 Most Similar Headlines:\\n\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank}. [Similarity: {similarities[idx]:.3f}] {news_headlines[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ec5f6",
   "metadata": {},
   "source": [
    "## Section 5: Visualization and Analysis\n",
    "\n",
    "Let's compare different models and analyze network properties in depth.\n",
    "\n",
    "### 5.1 Comparing Different Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two models: MiniLM (fast) vs MPNet (accurate)\n",
    "models = {\n",
    "    'MiniLM-L6': 'sentence-transformers/all-MiniLM-L6-v2',  # 384 dims, 23M params\n",
    "    'MPNet-base': 'sentence-transformers/all-mpnet-base-v2',  # 768 dims, 110M params\n",
    "}\n",
    "\n",
    "# Test sentences for comparison\n",
    "test_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on the rug\",\n",
    "    \"Dogs are loyal pets\",\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model_name in models.items():\n",
    "    print(f\"Testing {name}...\")\n",
    "    embedder_test = TransformerEmbeddings(model_name=model_name)\n",
    "    sim_matrix = embedder_test.compute_similarity_matrix(test_sentences)\n",
    "    results[name] = sim_matrix\n",
    "\n",
    "# Visualize side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, sim_matrix) in enumerate(results.items()):\n",
    "    sns.heatmap(\n",
    "        sim_matrix,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        xticklabels=['S1', 'S2', 'S3'],\n",
    "        yticklabels=['S1', 'S2', 'S3'],\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={'label': 'Similarity'}\n",
    "    )\n",
    "    axes[idx].set_title(f'{name}\\nDimensions: {results[name].shape[0]}')\n",
    "\n",
    "plt.suptitle('Model Comparison: Similarity Matrices', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"‚Ä¢ MiniLM is faster and more memory-efficient (384 dims)\")\n",
    "print(f\"‚Ä¢ MPNet is more accurate but slower (768 dims)\")\n",
    "print(f\"‚Ä¢ Both correctly identify S1 and S2 as highly similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded865e6",
   "metadata": {},
   "source": [
    "### 5.2 Network Analysis Metrics\n",
    "\n",
    "Let's analyze the properties of our document network using NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the news headlines network\n",
    "print(\"üìä Network Statistics:\")\n",
    "print(f\"   Nodes: {G_news.number_of_nodes()}\")\n",
    "print(f\"   Edges: {G_news.number_of_edges()}\")\n",
    "print(f\"   Density: {nx.density(G_news):.4f}\")\n",
    "print(f\"   Average Degree: {sum(dict(G_news.degree()).values()) / G_news.number_of_nodes():.2f}\")\n",
    "\n",
    "# Connected components\n",
    "num_components = nx.number_connected_components(G_news)\n",
    "print(f\"   Connected Components: {num_components}\")\n",
    "\n",
    "# Clustering coefficient\n",
    "avg_clustering = nx.average_clustering(G_news, weight='weight')\n",
    "print(f\"   Avg Clustering Coefficient: {avg_clustering:.4f}\")\n",
    "\n",
    "# Centrality measures\n",
    "degree_centrality = nx.degree_centrality(G_news)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_news, weight='weight')\n",
    "\n",
    "# Top 5 most central nodes\n",
    "print(\"\\nüèÜ Top 5 Most Central Documents (by degree):\")\n",
    "top_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for doc_id, centrality in top_central:\n",
    "    print(f\"   [{doc_id}] {news_headlines[doc_id][:60]}... (centrality: {centrality:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degree distribution\n",
    "degrees = [G_news.degree(n) for n in G_news.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(degrees, bins=range(max(degrees)+2), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Degree Distribution')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "weights = [G_news[u][v]['weight'] for u, v in G_news.edges()]\n",
    "plt.hist(weights, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Edge Weight (Similarity)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Edge Weight Distribution')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525035f",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ **Sentence Embeddings** - Transform text into dense vectors that capture semantic meaning  \n",
    "‚úÖ **Document Networks** - Connect similar documents based on embedding similarity  \n",
    "‚úÖ **Term Networks** - Identify semantically related concepts  \n",
    "‚úÖ **Community Detection** - Automatically group similar content  \n",
    "‚úÖ **Model Comparison** - Trade-offs between speed (MiniLM) and accuracy (MPNet)  \n",
    "‚úÖ **Network Analysis** - Measure centrality, clustering, and network properties\n",
    "\n",
    "### Advantages over Co-occurrence Methods:\n",
    "\n",
    "| Feature | Co-occurrence | Transformers |\n",
    "|---------|--------------|--------------|\n",
    "| **Semantic Understanding** | ‚ùå Word-level only | ‚úÖ Deep semantic meaning |\n",
    "| **Synonyms** | ‚ùå Treated as different | ‚úÖ Recognized as similar |\n",
    "| **Context** | ‚ö†Ô∏è Local window only | ‚úÖ Full context |\n",
    "| **Sparsity** | ‚ö†Ô∏è Very sparse | ‚úÖ Dense representations |\n",
    "| **Speed** | ‚úÖ Fast | ‚ö†Ô∏è Slower (but GPU helps) |\n",
    "| **Memory** | ‚úÖ Lower | ‚ö†Ô∏è Higher |\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**Use Co-occurrence** when:\n",
    "- You have very large datasets (millions of documents)\n",
    "- You care about exact word usage patterns\n",
    "- You need maximum speed\n",
    "- Memory is limited\n",
    "\n",
    "**Use Transformers** when:\n",
    "- You need semantic understanding\n",
    "- Dataset is medium-sized (<100K documents)\n",
    "- Quality is more important than speed\n",
    "- You have GPU access\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try on your own data with different similarity thresholds\n",
    "2. Experiment with multilingual models for non-English text\n",
    "3. Combine with co-occurrence methods for hybrid approaches\n",
    "4. Explore BERTopic for automated topic modeling\n",
    "\n",
    "Happy analyzing! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
