{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb2ccea",
   "metadata": {},
   "source": [
    "# Complete Workflow: From Text to Network Contagion Analysis\n",
    "\n",
    "This notebook demonstrates a full end-to-end workflow using the Network Inference Toolkit:\n",
    "\n",
    "1. **Data Loading & Preparation** - Load and clean text data\n",
    "2. **Network Building** - Build semantic network from co-occurrence\n",
    "3. **Network Analysis** - Analyze network structure and communities\n",
    "4. **Contagion Simulation** - Model information spread on the network\n",
    "5. **Visualization** - Visualize networks and simulation results\n",
    "\n",
    "**Dataset**: We'll use sample political discussion data, but this workflow applies to any text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d3fba",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preparation\n",
    "\n",
    "First, we'll load sample data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07593922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data if needed\n",
    "sample_texts = [\n",
    "    \"Climate change requires urgent policy action and international cooperation\",\n",
    "    \"Economic policy should focus on job creation and sustainable growth\",\n",
    "    \"Healthcare reform needs bipartisan support to improve access\",\n",
    "    \"Climate policy and environmental protection are critical priorities\",\n",
    "    \"International trade policy affects domestic job markets significantly\",\n",
    "    \"Sustainable development requires balancing economic and environmental goals\",\n",
    "    \"Healthcare access remains a pressing policy challenge nationwide\",\n",
    "    \"Job creation through infrastructure investment boosts economic growth\",\n",
    "    \"Environmental regulations impact both climate and economic policy\",\n",
    "    \"International cooperation on climate change shows mixed results\",\n",
    "] * 10  # Repeat to have more data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': sample_texts,\n",
    "    'id': range(len(sample_texts)),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=len(sample_texts), freq='H')\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"\\nSample data:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data quality checks\n",
    "print(\"Data Quality Summary:\")\n",
    "print(f\"  Missing values: {df['text'].isna().sum()}\")\n",
    "print(f\"  Empty strings: {(df['text'].str.len() == 0).sum()}\")\n",
    "print(f\"  Avg text length: {df['text'].str.len().mean():.1f} characters\")\n",
    "print(f\"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218605ee",
   "metadata": {},
   "source": [
    "## 2. Network Building\n",
    "\n",
    "We'll build a semantic network based on term co-occurrence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792981f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to temporary file for CLI processing\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "input_file = os.path.join(temp_dir, 'input.csv')\n",
    "output_dir = os.path.join(temp_dir, 'output')\n",
    "\n",
    "df.to_csv(input_file, index=False)\n",
    "print(f\"Data saved to: {input_file}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build semantic network using CLI\n",
    "!python -m src.semantic.build_semantic_network \\\n",
    "    --input {input_file} \\\n",
    "    --outdir {output_dir} \\\n",
    "    --min-df 2 \\\n",
    "    --topk 10 \\\n",
    "    --output-format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network results\n",
    "nodes_df = pd.read_csv(os.path.join(output_dir, 'nodes.csv'))\n",
    "edges_df = pd.read_csv(os.path.join(output_dir, 'edges.csv'))\n",
    "\n",
    "print(f\"Network Statistics:\")\n",
    "print(f\"  Nodes (terms): {len(nodes_df)}\")\n",
    "print(f\"  Edges (co-occurrences): {len(edges_df)}\")\n",
    "print(f\"  Avg degree: {2 * len(edges_df) / len(nodes_df):.2f}\")\n",
    "\n",
    "print(f\"\\nTop 10 terms by frequency:\")\n",
    "nodes_df.nlargest(10, 'count')[['token', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect edge weights\n",
    "print(\"Edge weight distribution:\")\n",
    "print(edges_df['weight'].describe())\n",
    "\n",
    "print(f\"\\nTop 10 strongest edges:\")\n",
    "edges_df.nlargest(10, 'weight')[['src', 'dst', 'weight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59c074",
   "metadata": {},
   "source": [
    "## 3. Network Analysis\n",
    "\n",
    "Analyze the network structure and detect communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network into NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes\n",
    "for _, row in nodes_df.iterrows():\n",
    "    G.add_node(row['id'], token=row['token'], count=row['count'])\n",
    "\n",
    "# Add edges with weights\n",
    "for _, row in edges_df.iterrows():\n",
    "    G.add_edge(row['src'], row['dst'], weight=row['weight'])\n",
    "\n",
    "print(f\"NetworkX Graph:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Connected: {nx.is_connected(G)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute network metrics\n",
    "print(\"Network Metrics:\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")\n",
    "print(f\"  Average clustering: {nx.average_clustering(G):.4f}\")\n",
    "\n",
    "if nx.is_connected(G):\n",
    "    print(f\"  Average path length: {nx.average_shortest_path_length(G):.2f}\")\n",
    "    print(f\"  Diameter: {nx.diameter(G)}\")\n",
    "else:\n",
    "    print(f\"  Connected components: {nx.number_connected_components(G)}\")\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    print(f\"  Largest component size: {len(largest_cc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e15213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect communities using Louvain\n",
    "from networkx.algorithms import community\n",
    "\n",
    "communities = community.louvain_communities(G, seed=42)\n",
    "print(f\"\\nCommunity Detection:\")\n",
    "print(f\"  Number of communities: {len(communities)}\")\n",
    "print(f\"  Community sizes: {[len(c) for c in communities]}\")\n",
    "\n",
    "# Assign community labels\n",
    "node_to_comm = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        node_to_comm[node] = i\n",
    "\n",
    "# Show community memberships\n",
    "for i, comm in enumerate(communities):\n",
    "    tokens = [G.nodes[n]['token'] for n in comm]\n",
    "    print(f\"\\nCommunity {i}: {', '.join(tokens[:10])}{'...' if len(tokens) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f0e50",
   "metadata": {},
   "source": [
    "## 4. Contagion Simulation\n",
    "\n",
    "Simulate information spread on the semantic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SI model simulation using CLI\n",
    "sim_output = os.path.join(temp_dir, 'simulation')\n",
    "\n",
    "!python -m src.contagion.cli \\\n",
    "    {os.path.join(output_dir, 'edges.csv')} \\\n",
    "    --model si \\\n",
    "    --beta 0.1 \\\n",
    "    --timesteps 50 \\\n",
    "    --initial-frac 0.05 \\\n",
    "    --seed 42 \\\n",
    "    --output-path {sim_output} \\\n",
    "    --output-format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation results\n",
    "sim_results = pd.read_csv(sim_output + '.csv')\n",
    "\n",
    "print(\"Simulation Summary:\")\n",
    "print(f\"  Total timesteps: {len(sim_results)}\")\n",
    "print(f\"  Initial infected: {sim_results.iloc[0]['infected']}\")\n",
    "print(f\"  Final infected: {sim_results.iloc[-1]['infected']}\")\n",
    "print(f\"  Adoption rate: {sim_results.iloc[-1]['infected'] / len(nodes_df):.2%}\")\n",
    "\n",
    "sim_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263abb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize contagion spread over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sim_results['timestep'], sim_results['infected'], linewidth=2)\n",
    "plt.xlabel('Timestep', fontsize=12)\n",
    "plt.ylabel('Number of Infected Nodes', fontsize=12)\n",
    "plt.title('SI Model: Information Spread on Semantic Network', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation: The curve shows how information spreads through the semantic network.\")\n",
    "print(f\"With beta={0.1}, the infection reaches {sim_results.iloc[-1]['infected']} out of {len(nodes_df)} terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93135",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Visualize the network structure and highlight important nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9db3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node centralities\n",
    "degree_cent = nx.degree_centrality(G)\n",
    "betweenness_cent = nx.betweenness_centrality(G)\n",
    "eigenvector_cent = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "\n",
    "# Create centrality DataFrame\n",
    "centrality_df = pd.DataFrame({\n",
    "    'node': list(G.nodes()),\n",
    "    'token': [G.nodes[n]['token'] for n in G.nodes()],\n",
    "    'degree': [degree_cent[n] for n in G.nodes()],\n",
    "    'betweenness': [betweenness_cent[n] for n in G.nodes()],\n",
    "    'eigenvector': [eigenvector_cent[n] for n in G.nodes()]\n",
    "})\n",
    "\n",
    "print(\"Top 10 most central terms (by degree centrality):\")\n",
    "centrality_df.nlargest(10, 'degree')[['token', 'degree', 'betweenness', 'eigenvector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd97538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Node sizes based on degree\n",
    "node_sizes = [300 * degree_cent[n] for n in G.nodes()]\n",
    "\n",
    "# Node colors based on community\n",
    "node_colors = [node_to_comm.get(n, 0) for n in G.nodes()]\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, \n",
    "                       cmap=plt.cm.Set3, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)\n",
    "\n",
    "# Label top nodes\n",
    "top_nodes = centrality_df.nlargest(10, 'degree')['node'].values\n",
    "labels = {n: G.nodes[n]['token'] for n in top_nodes}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=10, font_weight='bold')\n",
    "\n",
    "plt.title('Semantic Network with Communities', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization: Node size = degree centrality, color = community\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d616d",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis: Parameter Inference\n",
    "\n",
    "Infer optimal contagion parameters from observed cascade size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we observed a cascade that infected 60% of the network\n",
    "observed_size = int(0.6 * len(nodes_df))\n",
    "\n",
    "inference_output = os.path.join(temp_dir, 'inference')\n",
    "\n",
    "!python -m src.contagion.cli_inference \\\n",
    "    {os.path.join(output_dir, 'edges.csv')} \\\n",
    "    --model si \\\n",
    "    --observed-final-size {observed_size} \\\n",
    "    --beta-min 0.01 \\\n",
    "    --beta-max 0.5 \\\n",
    "    --n-samples 10 \\\n",
    "    --timesteps 50 \\\n",
    "    --seed 42 \\\n",
    "    --output-path {inference_output} \\\n",
    "    --output-format csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inference results\n",
    "trials_df = pd.read_csv(inference_output + '_trials.csv')\n",
    "best_params_df = pd.read_csv(inference_output + '_best_params.csv')\n",
    "\n",
    "print(\"Parameter Inference Results:\")\n",
    "print(best_params_df)\n",
    "\n",
    "# Visualize parameter search\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(trials_df['beta'], trials_df['score'], alpha=0.6, s=100)\n",
    "plt.xlabel('Beta (infection rate)', fontsize=12)\n",
    "plt.ylabel('Score (negative abs error)', fontsize=12)\n",
    "plt.title('Parameter Search: Beta vs Score', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight best\n",
    "best_beta = best_params_df.iloc[0]['beta']\n",
    "best_score = best_params_df.iloc[0]['score']\n",
    "plt.scatter([best_beta], [best_score], color='red', s=200, marker='*', \n",
    "           label=f'Best: β={best_beta:.3f}')\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest beta value: {best_beta:.3f} achieves final size closest to target of {observed_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d07094",
   "metadata": {},
   "source": [
    "## 7. Comparing Models\n",
    "\n",
    "Compare SI, SIS, and SIR models on the same network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4859b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple models\n",
    "models_to_test = [\n",
    "    ('si', {'beta': 0.1}),\n",
    "    ('sis', {'beta': 0.1, 'gamma': 0.05}),\n",
    "    ('sir', {'beta': 0.1, 'gamma': 0.05})\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, params in models_to_test:\n",
    "    model_output = os.path.join(temp_dir, f'{model_name}_results')\n",
    "    \n",
    "    cmd = f\"python -m src.contagion.cli {os.path.join(output_dir, 'edges.csv')} \"\n",
    "    cmd += f\"--model {model_name} --timesteps 50 --initial-frac 0.05 --seed 42 \"\n",
    "    cmd += f\"--output-path {model_output} --output-format csv \"\n",
    "    \n",
    "    for param, value in params.items():\n",
    "        cmd += f\"--{param} {value} \"\n",
    "    \n",
    "    !{cmd}\n",
    "    \n",
    "    # Load results\n",
    "    results[model_name] = pd.read_csv(model_output + '.csv')\n",
    "\n",
    "print(\"✓ All models simulated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f61ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models visually\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for model_name, df in results.items():\n",
    "    plt.plot(df['timestep'], df['infected'], label=model_name.upper(), linewidth=2)\n",
    "\n",
    "plt.xlabel('Timestep', fontsize=12)\n",
    "plt.ylabel('Number of Infected Nodes', fontsize=12)\n",
    "plt.title('Model Comparison: SI vs SIS vs SIR', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"- SI: Monotonic growth (no recovery)\")\n",
    "print(\"- SIS: Oscillates (nodes can be re-infected)\")\n",
    "print(\"- SIR: Peak then decline (immunity after recovery)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d68f0f",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. ✅ Loaded and prepared text data\n",
    "2. ✅ Built semantic network from co-occurrence patterns\n",
    "3. ✅ Analyzed network structure (centrality, communities)\n",
    "4. ✅ Simulated contagion spread (SI/SIS/SIR models)\n",
    "5. ✅ Inferred optimal parameters from observed cascades\n",
    "6. ✅ Visualized networks and simulation results\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Network has strong community structure around key topics\n",
    "- Information spreads efficiently through high-degree hubs\n",
    "- Different contagion models show distinct spread dynamics\n",
    "- Parameter inference can match observed cascade sizes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For More Advanced Analysis:**\n",
    "- Use transformer networks for semantic similarity (`transformers_cli`)\n",
    "- Build time-sliced networks to track evolution (`time_slice_cli`)\n",
    "- Try complex contagion models (Watts, K-Reinforcement)\n",
    "- Export to Gephi for interactive visualization\n",
    "\n",
    "**For Real Data:**\n",
    "- Scale up with larger datasets (10K+ documents)\n",
    "- Tune parameters (min_df, topk, similarity thresholds)\n",
    "- Use config files for reproducible workflows\n",
    "- Save results in Parquet format for efficiency\n",
    "\n",
    "**For Further Reading:**\n",
    "- See `README.md` for complete documentation\n",
    "- See `CONTAGION.md` for detailed contagion modeling guide\n",
    "- Check `examples/` for more specialized notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"✓ Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
